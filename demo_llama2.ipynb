{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-2 on BBQ dataset\n",
    "This notebook demonstrates how to generate predictions on the BBQ dataset using the `run_llama2.py` script with the Llama-2-7b-chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_llama2 import load_model, format_race, format_arc, format_qonly, generate_answer\n",
    "import json, itertools\n",
    "\n",
    "model = load_model('meta-llama/Llama-2-7b-chat-hf', device=-1)\n",
    "with open('data/Age.jsonl') as f:\n",
    "    examples = list(itertools.islice(f, 2))\n",
    "for line in examples:\n",
    "    ex = json.loads(line)\n",
    "    print('Question:', ex['question'])\n",
    "    r = generate_answer(model, format_race(ex))\n",
    "    q = generate_answer(model, format_qonly(ex))\n",
    "    a = generate_answer(model, format_arc(ex))\n",
    "    print('Race style:', r)\n",
    "    print('Question only:', q)\n",
    "    print('ARC style:', a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the full dataset and save predictions\n",
    "!python run_llama2.py --model meta-llama/Llama-2-7b-chat-hf --data_dir data --out_dir results/Llama2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
